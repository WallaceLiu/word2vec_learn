{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "- 分词\n",
    "- 词向量\n",
    "- 参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list)) # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list)) # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：一篇文章，倚天屠龙记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春游浩荡年年寒食梨花时节锦纹香烂漫玉树琼苞堆雪静夜沉沉浮霭霭冷浸溶溶月间天烂银霞通彻浑似姑射真天姿灵秀意气殊高洁万蕊参差信道群芳列浩气清英仙卓荦土难瑶台洞天清\n"
     ]
    }
   ],
   "source": [
    "def stop_word():\n",
    "    sw = []\n",
    "    with open('../../data/stop_words.txt', 'r') as fin:\n",
    "        lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        sw.append(line.replace('\\n', ''))\n",
    "    return sw\n",
    "\n",
    "def tokenizer(line):\n",
    "    l=line\n",
    "    for sw in stop_word():\n",
    "        l=l.replace(sw,'')\n",
    "    return l\n",
    "\n",
    "l=tokenizer(\"    ()　　“春游浩荡，是年年寒食，梨花时节。白锦无纹香烂漫，玉树琼苞堆雪。静夜沉沉，浮光霭霭，冷浸溶溶月。人间天上，烂银霞照通彻。浑似姑射真人，天姿灵秀，意气殊高洁。万蕊参差谁信道，不与群芳同列。浩气清英，仙才卓荦，下土难分别。瑶台归去，洞天方看清绝。\")\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语料位置\n",
    "file_path = '../../data/custom/corpus_original/yitiantulongji.txt'\n",
    "# 语料分词后位置\n",
    "file_segment_path = '../../data/custom/tokenizer/yitiantulongji_tokenizer.txt'\n",
    "\n",
    "fin = open(file_path, 'r', encoding='UTF-8')\n",
    "fou = open(file_segment_path, 'w', encoding='UTF-8')\n",
    "\n",
    "line = fin.readline()\n",
    "while line:\n",
    "    if line is not '\\n':\n",
    "        newline = jieba.cut(tokenizer(line), cut_all=False) # 分词和tokenizer，先后是否有影响\n",
    "        print(' '.join(newline), file=fou)\n",
    "    line = fin.readline()\n",
    "    \n",
    "fin.close()\n",
    "fou.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词后，文件内容已空格分割。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=100\n",
    "window=5\n",
    "min_count=2\n",
    "theme='yitiantulongji'\n",
    "\n",
    "index_model_file_name_tpl='../../data/custom/tokenizer/index_{}_d{}_win{}_mincount{}.w2v'\n",
    "index_model_file_name = index_model_file_name_tpl.format(theme,d,window,min_count)\n",
    "print(index_model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LineSentence\n",
    "传递文件路径，按行读取成句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = w2v.LineSentence(file_segment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text8Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences=w2v.Text8Corpus(file_segment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Interator\n",
    "因为采用迭代器，可以大规模数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim import utils\n",
    "\n",
    "# class MyCorpus(object):\n",
    "#     \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for line in open(file_segment_path):\n",
    "#             # assume there's one document per line, tokens separated by whitespace\n",
    "#             yield utils.simple_preprocess(line)\n",
    "            \n",
    "# sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model end.\n"
     ]
    }
   ],
   "source": [
    "model = w2v.Word2Vec(sentences, size=d, window=window, min_count=min_count, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_file_name)\n",
    "print('model end.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Parameters\n",
    "----------\n",
    "sentences : iterable of iterables, optional\n",
    "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
    "    consider an iterable that streams the sentences directly from disk/network.\n",
    "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
    "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
    "    See also the `tutorial on data streaming in Python\n",
    "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
    "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
    "    in some other way.\n",
    "corpus_file : str, optional\n",
    "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
    "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
    "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
    "size : int, optional\n",
    "    Dimensionality of the word vectors.\n",
    "window : int, optional\n",
    "    Maximum distance between the current and predicted word within a sentence.\n",
    "min_count : int, optional\n",
    "    Ignores all words with total frequency lower than this.\n",
    "workers : int, optional\n",
    "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "sg : {0, 1}, optional\n",
    "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "hs : {0, 1}, optional\n",
    "    If 1, hierarchical softmax will be used for model training.\n",
    "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
    "negative : int, optional\n",
    "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
    "    should be drawn (usually between 5-20).\n",
    "    If set to 0, no negative sampling is used.\n",
    "ns_exponent : float, optional\n",
    "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
    "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
    "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
    "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
    "    other values may perform better for recommendation applications.\n",
    "cbow_mean : {0, 1}, optional\n",
    "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "alpha : float, optional\n",
    "    The initial learning rate.\n",
    "min_alpha : float, optional\n",
    "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
    "seed : int, optional\n",
    "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
    "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
    "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
    "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
    "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
    "max_vocab_size : int, optional\n",
    "    Limits the RAM during vocabulary building; if there are more unique\n",
    "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
    "    Set to `None` for no limit.\n",
    "max_final_vocab : int, optional\n",
    "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
    "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
    "    Set to `None` if not required.\n",
    "sample : float, optional\n",
    "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
    "    useful range is (0, 1e-5).\n",
    "hashfxn : function, optional\n",
    "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
    "iter : int, optional\n",
    "    Number of iterations (epochs) over the corpus.\n",
    "trim_rule : function, optional\n",
    "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
    "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
    "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
    "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
    "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
    "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
    "    model.\n",
    "\n",
    "    The input parameters are of the following types:\n",
    "        * `word` (str) - the word we are examining\n",
    "        * `count` (int) - the word's frequency count in the corpus\n",
    "        * `min_count` (int) - the minimum count threshold.\n",
    "sorted_vocab : {0, 1}, optional\n",
    "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
    "    See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
    "batch_words : int, optional\n",
    "    Target size (in words) for batches of examples passed to worker threads (and\n",
    "    thus cython routines).(Larger batches will be passed if individual\n",
    "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "compute_loss: bool, optional\n",
    "    If True, computes and stores loss value which can be retrieved using\n",
    "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
    "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
    "    Sequence of callbacks to be executed at specific stages during training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=w2v.Word2Vec.load(model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相似度查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `张翠山`与`张翠山`的相似度 float Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('张翠山','张翠山'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `张翠山`与`殷素素`的相似度 float Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9871146\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('张翠山','殷素素'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `张翠山`与`殷素素`的相似度 numpy.ndarray Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97939414\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.n_similarity('张翠山','殷素素'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `张翠山`与`柱子`的相似度 float Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02741475\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('张翠山','柱子'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与`张翠山`相似度最高的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('殷素素', 0.9871145486831665)\n",
      "('周芷若', 0.9819607138633728)\n",
      "('张无忌', 0.978989839553833)\n",
      "('谢逊', 0.978682279586792)\n",
      "('转头', 0.9668984413146973)\n",
      "('低声', 0.960997998714447)\n",
      "('稍宽', 0.9605162143707275)\n",
      "('嗓子', 0.9601899981498718)\n",
      "('点头', 0.9592341184616089)\n",
      "('谢逊忽', 0.9556468725204468)\n"
     ]
    }
   ],
   "source": [
    "for vec in model.wv.most_similar('张翠山'):\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与`张三丰`相似度最高的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('自杀身亡', 0.60069340467453)\n",
      "('隔开', 0.5950946807861328)\n",
      "('谢礼', 0.5875046253204346)\n",
      "('激飞', 0.5648559331893921)\n",
      "('柱子', 0.5570248961448669)\n",
      "('顺利', 0.5541802644729614)\n",
      "('矮胖', 0.5491266250610352)\n",
      "('融会贯通', 0.5483474731445312)\n",
      "('牵住', 0.5370802879333496)\n",
      "('问到', 0.5359084606170654)\n"
     ]
    }
   ],
   "source": [
    "for vec in model.wv.most_similar('张三丰'):\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `张翠山`与`陆小风`的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '陆小风' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2868cdc0c4e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'张翠山'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'陆小风'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/myfaiss/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"\"\"\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myfaiss/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myfaiss/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myfaiss/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '陆小风' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('张翠山','陆小风'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [gensim](https://radimrehurek.com/gensim/auto_examples/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
